{"meta":{"title":"jiarus","subtitle":null,"description":null,"author":"fever","url":"https://jiarus.github.io","root":"/"},"pages":[{"title":"","date":"2019-08-11T03:13:12.174Z","updated":"2019-08-11T03:13:12.174Z","comments":true,"path":"404.html","permalink":"https://jiarus.github.io/404.html","excerpt":"","text":"404"},{"title":"about","date":"2019-07-27T21:07:34.000Z","updated":"2019-08-03T10:26:34.775Z","comments":true,"path":"1/index.html","permalink":"https://jiarus.github.io/1/index.html","excerpt":"","text":"test"},{"title":"关于","date":"2019-08-11T03:40:09.779Z","updated":"2019-08-11T03:40:09.779Z","comments":false,"path":"about/index.html","permalink":"https://jiarus.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2019-08-11T03:58:56.766Z","updated":"2019-08-11T03:58:56.766Z","comments":false,"path":"books/index.html","permalink":"https://jiarus.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-08-11T03:34:59.052Z","updated":"2019-08-11T03:34:59.052Z","comments":false,"path":"categories/index.html","permalink":"https://jiarus.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-08-11T04:11:54.345Z","updated":"2019-08-11T04:11:54.345Z","comments":true,"path":"links/index.html","permalink":"https://jiarus.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-08-11T04:25:08.512Z","updated":"2019-08-11T04:25:08.512Z","comments":false,"path":"repository/index.html","permalink":"https://jiarus.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-08-11T03:36:42.036Z","updated":"2019-08-11T03:36:42.036Z","comments":false,"path":"tags/index.html","permalink":"https://jiarus.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"计算机CPU性能提升","slug":"计算机CPU性能提升","date":"2019-07-27T20:16:00.000Z","updated":"2019-08-11T02:36:48.123Z","comments":true,"path":"2019/07/28/计算机CPU性能提升/","link":"","permalink":"https://jiarus.github.io/2019/07/28/计算机CPU性能提升/","excerpt":"","text":"CPU提升的途径：增加晶体管密度、提升CPU主频（晶体管开关的速度） 增加晶体管密度就需要把晶体管造的更小，这就是所谓的”制程” CPU提升带来的是功率的增加： CPU功率 ～=1/2x负载电容x电压的平方x开关频率x晶体管数量 *降低CPU电压最容易提升计算机的续航* 但仅靠提升CPU性能会遇到瓶颈，就需要采用CPU并行提高性能 并行也会需要瓶颈，正如阿姆达尔定律（Amdahl’s Law） 优化后的执行时间 = 受优化影响的执行时间 / 加速倍数 + 不受影响的执行时间 一个程序可以分解为多个CPU的任务来处理，处理结束需要将多个CPU处理结果汇总在一起 100/4 + 20 = 45100/100 + 20 = 21 可以看出仅靠增加CPU是会遇到瓶颈的，不受影响的执行时间无法做到并行处理","categories":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://jiarus.github.io/categories/计算机组成原理/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://jiarus.github.io/tags/计算机组成原理/"}],"author":"fever"},{"title":"计算机性能之CPU篇","slug":"计算机性能之CPU篇","date":"2019-07-27T19:37:00.000Z","updated":"2019-08-11T02:37:04.064Z","comments":true,"path":"2019/07/28/计算机性能之CPU篇/","link":"","permalink":"https://jiarus.github.io/2019/07/28/计算机性能之CPU篇/","excerpt":"","text":"Keyword:响应时间、吞吐率响应时间主要依靠提升CPU性能、吞吐率可以多增加几台机器计算性能衡量：1/响应时间 统计从1到100w需要花费的时间time seq 1000000 | wc -l 1000000 real 0m0.101s //系统真正花费的时间 user 0m0.031s //在用户态花费的时间 sys 0m0.016s //程序花费的时间 有可能real &lt; user + sys ，这是因为系统是多个CPU的情况，user+sys统计的在多个CPU上一共花费的时间，而real是现实中时钟过去的时间 *CPU执行时间 = user + sys = CPU时钟周期数 x CPU时钟周期时间CPU时钟周期时间：计算机的主频（2.8GHz），是计算机CPU中晶体振荡器（晶振）滴答一次的时间 = 1/2.8GHzCPU时钟周期数 ：指令数 x 每条指令的平均时钟周期数（每条指令平均花费的时间,Cycles Per Instruction简称CPI）CPU执行时间 = 指令数 x CPI x 2.8GHz**结论：要提高程序的执行效率，需要从CPU的主频、单个时钟周期时间内能执行的指令数和指令总条数上来优化","categories":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://jiarus.github.io/categories/计算机组成原理/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://jiarus.github.io/tags/计算机组成原理/"}],"author":"fever"},{"title":"Kafka版本号","slug":"Kafka版本号总结","date":"2019-07-26T16:30:00.000Z","updated":"2019-08-04T20:02:37.221Z","comments":true,"path":"2019/07/27/Kafka版本号总结/","link":"","permalink":"https://jiarus.github.io/2019/07/27/Kafka版本号总结/","excerpt":"","text":"kafka目前总共演进了7个大版本，分别是0.7、0.8、0.9、0.11、1.0和2.01.0以前都是4位版本号，之后改为3位版本号。如今kafka已经发行到2.3.0版本了Scala 2.11代表的是Scala编译器的版本 0.7v 只有基本的消息队列功能，不包含副本机制 0.8v 引入了副本机制，正式进化为高可用的分布式消息队列解决方案。0.8.2.0之前的客户端API需要制定zookeeper地址，而不是broker地址。 producerAPI默认使用同步方式发送消息，异步可能会出现丢失消息的情况。建议升级到0.8.2.2 0.9v 2015年发布的0.9.0.0版本，增加了基础安全认证和权限功能 使用Java重写了消费者客户端（但存在很多BUG） 0.10v 主要变更都是在kafka Streams组件上 0.11v 引入幂等性producerAPI和事务API 对kafka消息格式重构 这个版本各个组件都非常稳定了。建议使用0.11.0.3，这个是国内最主流的版本。","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"使用一维数组存储二叉树","slug":"使用一维数组存储二叉树","date":"2019-07-10T03:58:00.000Z","updated":"2019-08-10T03:59:41.632Z","comments":true,"path":"2019/07/10/使用一维数组存储二叉树/","link":"","permalink":"https://jiarus.github.io/2019/07/10/使用一维数组存储二叉树/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546/*** * 需求：使用一维数组存储二叉树 * 步骤： * 1、查看原始数据的个数（8个），从而制定二叉树层级（4层），得到满二叉树节点个数（15个） * 2、二叉树节点（15个）为一维数组，全设置为0 * 3、循环遍历原始数据，第一个值为树根 * 4、第二个值与父节点比较，如果大于树根，则往右子树比较，如果数组内的值小于或等于树根，则往左子树比较 * 5、【循环】步骤4，直到形成二叉树 * * 备注：左节点的坐标等于父节点的坐标*2，右节点的坐标等于父节点的坐标*2+1 */import java.io.*;public class binaryTree&#123; public static void main(String args[]) throws IOException &#123; int i,level; int data[]=&#123;6,3,5,9,7,8,4,2&#125;; /*原始数组*/ int btree[]=new int[16]; for(i=0;i&lt;16;i++) btree[i]=0; System.out.print(&quot;原始数组内容: \\n&quot;); for(i=0;i&lt;8;i++) System.out.print(&quot;[&quot;+data[i]+&quot;] &quot;); System.out.println(); for(i=0;i&lt;8;i++) /*把原始数组中的值逐一对比*/ &#123; System.out.println(&quot;i==&gt;&quot;+i); for(level=1;btree[level]!=0;) /*比较树根及数组内的值*/ &#123; System.out.println(&quot;levele==&gt;&quot;+level+&quot; btree[level]==&gt;&quot;+btree[level]); if(data[i]&gt;btree[level]) /*如果数组内的值大于树根，则往右子树比较*/ level=level*2+1; else /*如果数组内的值小于或等于树根，则往左子树比较*/ level=level*2; &#125; /*如果子树节点的值不为0，则再与数组内的值比较一次*/ btree[level]=data[i]; /*把数组值放入二叉树*/ &#125; System.out.print(&quot;二叉树内容：\\n&quot;); for (i=1;i&lt;16;i++) System.out.print(&quot;[&quot;+btree[i]+&quot;] &quot;); System.out.print(&quot;\\n&quot;); &#125; &#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://jiarus.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://jiarus.github.io/tags/算法/"}],"author":"fever"},{"title":"Java并发包常用类用法及原理","slug":"Java并发包常用类用法及原理","date":"2019-06-02T04:33:00.000Z","updated":"2019-08-08T19:29:08.166Z","comments":true,"path":"2019/06/02/Java并发包常用类用法及原理/","link":"","permalink":"https://jiarus.github.io/2019/06/02/Java并发包常用类用法及原理/","excerpt":"","text":"com.java.util.concurrent包是java5时添加的，专门处理多线程提供的工具类 一、Atomic二、Lock三、BlockingQueue四、BlockDeque五、ConcurrnetMap六、CountDownLatch七、CyclicBarrier八、ExecutorService九、ForkJoinPool 1.atomic包 AtomicBoolean、AtomicInteger、AtomicLong、AtomicReference类提供多种方法，可以原子性地为参数取值、赋值、交换值（getAndSet）、比较并且设置值（CAS:compareAndSet）等。 为什么需要使用atomic？先说几个概念： 重排序 Java优化程序性能，在编译、处理和内存中对代码进行重排序，重排序是对代码的执行顺序做了修改。 happens-before 规定在编译、处理、内存中对代码执行重排序的规则。 as-if-serial语义 规定单线程中重排序和顺序执行的结果一致。 至于为什么需要在多线程使用atomic，有以下几点原因： 1.多个线程不能保证哪个线程先执行。因为不可见主内存值的问题，可能出现脏读的情况。 2.多线程赋值过程非原子性。因为变量在多线程中，修改一个主内存中的值，需要执行多个步骤（读取主内存，放入寄存器，修改值、赋值到主内存中），这么一来可能你在执行这个步骤的过程中，该变量被其他线程修改了，不能保证原子性。 3.使用volatile修饰变量。volatile是变量具有可见性（当寄存器中修改了值，会立即通知主内存和其他寄存器修改值）和有序性，查看AtomicInteger的源码，发现变量被volatile修饰。而原子性是Atomic中使用CAS（修改前判断主内存的值是否和当前的值一致）实现的，可见性就解决了上面问题1的脏读，有序性和原子性就解决了问题2中的问题。 12345private volatile int value;public AtomicInteger(int initialValue) &#123; value = initialValue;&#125; 2.locks ReentrantLock 可重入的互斥锁，即同一线程可以多次获得该锁，线程间是互斥的。 ReentrantReadWriteLock 可重入的读写锁，是在ReentrantLock的基础上的增强，更细粒度地控制。在特殊场景中会使用到，分为readLock和writeLock，读读共享，读写和写写排他。 Synchronized加锁实现原理： Synchronized经过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 Synchronized和ReentrantLock的区别联系： 相同点：都是加锁实现阻塞式的同步，一个线程获取了锁，其他线程就必须等待。 不同点： 使用上。Synchronized修饰方法和对象，ReentrantLock需要实例化，并且显示地调用lock()加锁和unlock()解锁。 等待可中断。ReentrantLock可以使用lockInterruptibly()方法中断锁或者设置超时中断。 公平锁。Synchronized是非公平锁，ReentrantLock默认也是非公平锁，可以指定为公平锁。 使用Condition条件 在Condition中，用await()替换wait()，用signal()替换notify()，用signalAll()替换notifyAll()，传统线程的通信方式，Condition都可以实现，这里注意，Condition是被绑定到Lock上的，要创建一个Lock的Condition必须用newCondition()方法。 3.BlockingQueue 提供了不同的插入移除检查方法，可以支持不同的返回值。 抛异常 特定值 阻塞 超时 插入 add(o) offer(o) put(o) offer(o, timeout, timeunit) 移除 remove(o) poll(o) take(o) poll(timeout, timeunit) 检查 get(o) peek(o) / / 阻塞队列值提供一个队列可以提供遵守FIFO放入和取出的操作，如果队列满了放入就会阻塞，相反队列如果为空，取出就会阻塞。 BlockingQueue是一个接口类，具体有多种实现，主要介绍5种常用的： ArrayBlockingQueue 数组阻塞队列，故名思议是用数组实现的阻塞队列，是有界的，只能在初始化确定队列容量大小。内部只有一个reentrantLock，读和写使用同一个锁，因此效率不高。 LinkedBlockingQueue 链表阻塞队列，顾名思义是用链表实现的阻塞队列，但它可以是有界的也可以无界的，内部有两个reentrantLock，读写锁是分离的。性能要比ArrayBlockingQueue要高。但创建和销毁Node，高并发对GC有一定压力。 12345678910//默认的构造器public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE);&#125;//指定容量的构造器public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; PriorityBlockingQueue 优先级阻塞队列。基于最小二叉堆实现，线程安全的无界队列。构造器中可以传入初始值和比较器的规则。根据比较器规则对内部元素排序。 SynchronousQueue 同步队列。内部只能存放一个元素。如果满了就插入就阻塞，相反如果为空取出就阻塞。 DelayQueue 延迟队列无界队列。内部使用优先级阻塞队列实现，只有元素过期才能取出来。并且按过期长短排序，队头的是过期最长的元素。使用ReentrantLock实现线程安全。 4.BlockDeque 提供了不同的插入移除检查方法，可以支持不同的返回值。 抛异常 特定值 阻塞 超时 插入 addFirst(o) offerFirst(o) putFirst(o) offerFirst(o, timeout, timeunit) 移除 removeFirst(o) pollFirst(o) takeFirst(o) pollFirst(timeout, timeunit) 检查 getFirst(o) peekFirst(o) / / LinkedBlockingDeque 双端链式阻塞队列。默认是无界的，也可以指定容量。该阻塞队列同时支持FIFO和FILO两种操作方式，队头和队尾都可以执行插入取出的操作。使用一把锁+两个条件维持队列的同步，和ArrayBlockingQueue的原理一样。 5.ConcurrentMap 支持并发操作的Map。 ConcurrentHashMap 是ConcurrentMap的具体实现。 1.发展。JDK1.7及之前都是使用Segment分段锁来实现的，因为Segment数量会限制并发量，而且在寻址也会执行两次hash，JDK1.8后取消Segment改为数组+链表+红黑树和CAS原子操作+synchronized实现。 2.初始化参数。 initialCapacity初始化Map的容量 loadFactor负载因子 concurrencyLevel是最好情况下可以达到的并发数（如果都访问的不同的Segment上）。Segment的个数是大于等于的第一个2的n次方的数,即设置15。即Segment = concurrencyLevel = 24 = 16。默认情况下，initialCapacity等于16，loadFactor等于0.75，concurrencyLevel等于16. 3.关于锁 1.7 Get没有加锁，因为Map中的key,value,nextHashEntry都是使用volatile修饰符修饰，多线程具有可见行。但是会进行两次Hash()方法寻址，第一次确定Segment位置，第二次确定table数组中位置。 Put使用的分段锁继承来ReentrantLock实现可重入锁。 1.8 Get方法同1.7相似都是没有加锁，一次hash寻址。 Put方法。使用CAS无锁机制，仅在Hash冲突时候加了synchronized同步锁。 4.扩容 数组容量增加一倍，并迁移链表中的数据 6.CountDownLatch 倒计时控制器(自己起的名字)。因为他类似于一个倒计时启动的功能。初始化指定倒计时的值CountDownLatch latch = new CountDownLatch(3)并使用latch.await()等待执行，当其他其他线程调用3次latch.countDown()就触发主线程继续。 7.CyclicBarrier 栅栏。允许定义N个线程到达栅栏才执行某个方法。 12//创建一个栅栏，这里设置2个线程都执行barrier1.await()方法后可以执行barrier1Action方法CyclicBarrier barrier1 = new CyclicBarrier(2, barrier1Action); 8.ExecutorService线程池服务接口，有两种具体的实现方式 ThreadPoolExecutor 序号 名称 类型 含义 1 corePoolSize int 核心线程池大小 2 maximumPoolSize int 最大线程池大小 3 keepAliveTime long 线程最大空闲时间 4 unit TimeUnit 时间单位 5 workQueue BlockingQueue 线程等待队列 6 threadFactory ThreadFactory 线程创建工厂 7 handler RejectedExecutionHandler 拒绝策略 实际上Executors类使用上述参数为他提供了多种预定义的实现。 简单介绍几种预定义实现： 1.FixedThreadPool:可以指定固定数量的核心线程，但是队列使用LinkedBlockingQueue是无界的，可能导致内存溢出。 2.CachedThreadPool:不限制线程的个数，要设置线程生存的周期，超过这个时间没有使用将自动回收线程。但是队列使用的是SynchronousQueue入队时必须出队。因为这些特性，该线程池应该用于类似于Netty中的短连接，快速处理大量耗时短的任务。 3.newSingleThreadExecutor:只创建一个线程，但是队列使用LinkedBlockingQueue无界队列。 ScheduledThreadPoolExecutor 继承了ThreadPoolExecutor，可以设置核心和最大线程的大小，使用DelayedWorkQueue延迟队列。 9.ForkJoinPool 实现了Executor接口，支持将一个大任务分为若干个子任务交给子线程处理，然后合并为一个结果集。采用了分治和递归的思想。内部维护了多个队列。（挖坑以后用到了再详细写）","categories":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/tags/java/"}],"author":"fever"},{"title":"布隆过滤器","slug":"布隆过滤器","date":"2019-05-08T07:00:00.000Z","updated":"2019-08-10T07:05:45.033Z","comments":true,"path":"2019/05/08/布隆过滤器/","link":"","permalink":"https://jiarus.github.io/2019/05/08/布隆过滤器/","excerpt":"","text":"判断一个字符串存不存在,可以用来过滤非法字符串、垃圾邮件等 1.使用BigSet存放hash位置信息 初始化定义存放位置size,如4、8、16、32 有多少个size就需要多少次hash，将得到的hash值作为索引存放在set中，value为true 2.判断是否存在，即将该字符串做hash处理，判断Bigset每个位置都为true，那么就认定该字符串存在 3.内存回收，当set占满，即每个字符串判断时都会存在，所以不能占满。可以设置为百分之80，清空该set 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201package com.onedesk.dsp.utils;import java.io.*;import java.util.BitSet;import java.util.concurrent.atomic.AtomicInteger;public class BloomFilter implements Serializable &#123; private static final long serialVersionUID = -5221305273707291280L; private final int[] seeds; private final int size; private final BitSet notebook; private final MisjudgmentRate rate; private final AtomicInteger useCount = new AtomicInteger(0); private final Double autoClearRate; /** * 默认中等程序的误判率：MisjudgmentRate.MIDDLE 以及不自动清空数据（性能会有少许提升） * * @param dataCount 预期处理的数据规模，如预期用于处理1百万数据的查重，这里则填写1000000 */ public BloomFilter(int dataCount) &#123; this(MisjudgmentRate.MIDDLE, dataCount, 0.8); &#125; /** * @param rate 一个枚举类型的误判率 * @param dataCount 预期处理的数据规模，如预期用于处理1百万数据的查重，这里则填写1000000 * @param autoClearRate 自动清空过滤器内部信息的使用比率，传null则表示不会自动清理， * 当过滤器使用率达到100%时，则无论传入什么数据，都会认为在数据已经存在了 * 当希望过滤器使用率达到80%时自动清空重新使用，则传入0.8 */ public BloomFilter(MisjudgmentRate rate, int dataCount, Double autoClearRate) &#123; long bitSize = rate.seeds.length * dataCount; if (bitSize &lt; 0 || bitSize &gt; Integer.MAX_VALUE) &#123; throw new RuntimeException(&quot;位数太大溢出了，请降低误判率或者降低数据大小&quot;); &#125; this.rate = rate; seeds = rate.seeds; size = (int) bitSize; notebook = new BitSet(size); this.autoClearRate = autoClearRate; &#125; public void add(String data) &#123; checkNeedClear(); for (int i = 0; i &lt; seeds.length; i++) &#123; int index = hash(data, seeds[i]); setTrue(index); &#125; &#125; public boolean check(String data) &#123; for (int i = 0; i &lt; seeds.length; i++) &#123; int index = hash(data, seeds[i]); if (!notebook.get(index)) &#123; return false; &#125; &#125; return true; &#125; /** * 如果不存在就进行记录并返回false，如果存在了就返回true * * @param data * @return */ public boolean addIfNotExist(String data) &#123; checkNeedClear(); int[] indexs = new int[seeds.length]; // 先假定存在 boolean exist = true; int index; for (int i = 0; i &lt; seeds.length; i++) &#123; indexs[i] = index = hash(data, seeds[i]); if (exist) &#123; if (!notebook.get(index)) &#123; // 只要有一个不存在，就可以认为整个字符串都是第一次出现的 exist = false; // 补充之前的信息 for (int j = 0; j &lt;= i; j++) &#123; setTrue(indexs[j]); &#125; &#125; &#125; else &#123; setTrue(index); &#125; &#125; return exist; &#125; private void checkNeedClear() &#123; if (autoClearRate != null) &#123; if (getUseRate() &gt;= autoClearRate) &#123; synchronized (this) &#123; if (getUseRate() &gt;= autoClearRate) &#123; notebook.clear(); useCount.set(0); &#125; &#125; &#125; &#125; &#125; public void setTrue(int index) &#123; useCount.incrementAndGet(); notebook.set(index, true); &#125; private int hash(String data, int seeds) &#123; char[] value = data.toCharArray(); int hash = 0; if (value.length &gt; 0) &#123; for (int i = 0; i &lt; value.length; i++) &#123; hash = i * hash + value[i]; &#125; &#125; hash = hash * seeds % size; // 防止溢出变成负数 return Math.abs(hash); &#125; public double getUseRate() &#123; return (double) useCount.intValue() / (double) size; &#125; /** * 清空过滤器中的记录信息 */ public void clear() &#123; useCount.set(0); notebook.clear(); &#125; public MisjudgmentRate getRate() &#123; return rate; &#125; /** * 分配的位数越多，误判率越低但是越占内存 * &lt;p&gt; * 4个位误判率大概是0.14689159766308 * &lt;p&gt; * 8个位误判率大概是0.02157714146322 * &lt;p&gt; * 16个位误判率大概是0.00046557303372 * &lt;p&gt; * 32个位误判率大概是0.00000021167340 * * @author lianghaohui */ public enum MisjudgmentRate &#123; // 这里要选取质数，能很好的降低错误率 /** * 每个字符串分配4个位 */ VERY_SMALL(new int[]&#123;2, 3, 5, 7&#125;), /** * 每个字符串分配8个位 */ SMALL(new int[]&#123;2, 3, 5, 7, 11, 13, 17, 19&#125;), // /** * 每个字符串分配16个位 */ MIDDLE(new int[]&#123;2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53&#125;), // /** * 每个字符串分配32个位 */ HIGH(new int[]&#123;2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131&#125;); private int[] seeds; private MisjudgmentRate(int[] seeds) &#123; this.seeds = seeds; &#125; public int[] getSeeds() &#123; return seeds; &#125; public void setSeeds(int[] seeds) &#123; this.seeds = seeds; &#125; &#125; public static void main(String[] args) &#123; BloomFilter fileter = new BloomFilter(100000); System.out.println(fileter.addIfNotExist(&quot;1111111111111&quot;)) &#125;&#125;","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"https://jiarus.github.io/tags/算法/"}],"author":"fever"},{"title":"Kakfa主题管理","slug":"Kakfa主题管理","date":"2019-04-26T12:13:00.000Z","updated":"2019-08-06T19:57:16.707Z","comments":true,"path":"2019/04/26/Kakfa主题管理/","link":"","permalink":"https://jiarus.github.io/2019/04/26/Kakfa主题管理/","excerpt":"","text":"创建Topic 1bin/kafka-topics.sh --bootstrap-server broker_host:port --create --topic my_topic_name --partitions 1 --replication-factor 1 kafka2.2之后推荐使用--bootstrap-server代替--zookeeper，因为通过前者创建可以控制权限，只和Broker打交道也是官方之后标准。 查看Topic列表 1bin/kafka-topics.sh --bootstrap-server broker_host:port --list 查看Topic详情 1bin/kafka-topics.sh --bootstrap-server broker_host:port --describe --topic &lt;topic_name&gt; 删除Topic 1bin/kafka-topics.sh --bootstrap-server broker_host:port --delete --topic &lt;topic_name&gt; 增加Topic分区 1bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic &lt;topic_name&gt; --partitions &lt; 新分区数 &gt; kafka不允许增加分区。因为多个broker节点都冗余有分区的数据，减少分区数需要操作多个broker且需要迁移该分区数据到其他分区。如果是按消息key hash选的分区，那么迁移就不知道迁到哪里了，因为只有业务代码可以决定放在哪。已经被大佬肯定了，应该没错。:) 修改Topic参数 1bin/kafka-configs.sh --zookeeper zookeeper_host:port --entity-type topics --entity-name &lt;topic_name&gt; --alter --add-config max.message.bytes=10485760 设置允许最大消息大小 变更Topic副本数 使用kafka-reassign-partitions 修改Topic限速 1bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config &apos;leader.replication.throttled.rate=104857600,follower.replication.throttled.rate=104857600&apos; --entity-type brokers --entity-name 0 主要限制Leader和Follower的副本使用的带宽。broker 0 代表某一个节点，多个需要每个都单独执行命令。 1bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config &apos;leader.replication.throttled.replicas=*,follower.replication.throttled.replicas=*&apos; --entity-type topics --entity-name test 同时还要设置要限速的副本*代表所有副本 内部Topic__consumer_offsets是用来记录consumer的offset值，默认创建50个分区。不需要手动管理。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"Golang中遍历非指针对象的坑","slug":"Golang中遍历非指针对象的坑","date":"2019-04-20T10:47:00.000Z","updated":"2019-08-11T00:56:51.988Z","comments":true,"path":"2019/04/20/Golang中遍历非指针对象的坑/","link":"","permalink":"https://jiarus.github.io/2019/04/20/Golang中遍历非指针对象的坑/","excerpt":"","text":"前段时间写了个递归树，中间遇到了个坑，按逻辑看是没问题，没想到结果不对。 for _, v := range tree我在遍历这个tree数组的时候传递的是数组对象，修改了数组的内容但返回时发现没有变化。这就是golang和java的差别，遍历的时候如果是非指针对象，那么golang会copy一个副本v，你修改的只是v的值，除非你修改完再重新放入数组，否则结果是不会变的。或者使用指针传递，使用数组的指针就没问题了。 这应该是golang和java思想的不同之处，golang处处都应该尽量使用指针，特别是大对象。 1234567891011121314151617181920212223242526272829303132333435363738func genGeoTree(arr *[]entity.GeoCommon) []*entity.GeoTree &#123; root := buildGeoRoot(arr) buildGeoChildren(arr, root) return root&#125;func buildGeoRoot(arr *[]entity.GeoCommon) []*entity.GeoTree &#123; var pNodes []*entity.GeoTree for _, v := range *arr &#123; if v.PCode == 0 &#123; var child entity.GeoTree child.Code = v.Code child.Name = v.Name child.Sort = v.Sort child.PCode = v.PCode pNodes = append(pNodes, &amp;child) &#125; &#125; return pNodes&#125;func buildGeoChildren(arr *[]entity.GeoCommon, tree []*entity.GeoTree) &#123; for _, v := range tree &#123; pNodes := make([]*entity.GeoTree, 0) for _, vv := range *arr &#123; if vv.PCode == v.Code &#123; var child entity.GeoTree child.Code = vv.Code child.Name = vv.Name child.Sort = vv.Sort child.PCode = vv.PCode pNodes = append(pNodes, &amp;child) buildGeoChildren(arr, pNodes) &#125; &#125; v.Children = pNodes &#125;&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://jiarus.github.io/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://jiarus.github.io/tags/golang/"}],"author":"fever"},{"title":"Kafka消费者组和Rebalance","slug":"Kafka消费者组","date":"2019-04-18T03:05:00.000Z","updated":"2019-08-06T03:32:21.402Z","comments":true,"path":"2019/04/18/Kafka消费者组/","link":"","permalink":"https://jiarus.github.io/2019/04/18/Kafka消费者组/","excerpt":"","text":"消费者组 Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制 特性： 1.一个组下可以包含多个消费者实例。可以是消费者进程，也可以是消费者线程。 2.GroupID是个字符串，标识唯一的group。 3.Topic下的分区只能被某个Group的一个Consumer消费。一个Consumer可以消费多个分区，即分区和消费者是多对一的关系。所以Kafka可以实现消息队列（一个消费者属于单个Group）,也可以实现发布/订阅模型（一个消费者属于多个Group）。 group中的offset老版本中保存在Zookeeper中，新版本保存在Broker节点Topic中。 Rebalance Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区。 触发条件: 1.Group下的consumer成员变更 2.Topic变更 3.Topic分区变更 影响： Stop The World.即在Rebalance时kafka会停止所有的服务，因为当前版本的Kafka触发Rebalance时候会重新分配所有的Consumer对应的分区，并不是像一致性哈希（一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。）那样尽量保证其他节点不影响。所以要尽量避免发生Rebalance的发生。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"Kafka幂等性和事务","slug":"Kafka幂等性和事务","date":"2019-04-07T05:42:00.000Z","updated":"2019-08-06T02:04:27.552Z","comments":true,"path":"2019/04/07/Kafka幂等性和事务/","link":"","permalink":"https://jiarus.github.io/2019/04/07/Kafka幂等性和事务/","excerpt":"","text":"幂等性 幂等性值多次执行某个操作，每次执行的结果都和第一次一样。Kafka可以在Producer端设置幂等性，0.11.0.0之后加入的新功能，enable.idempotence = true设置后Producer自动会对你的消息进行去重。但是只能保证单分区上的幂等性，即一个幂等性Producer只能保证某个Topic下的一个分区内不会出现重复数据，无法实现多分区幂等性。还有只能保证单会话幂等性，Producer重启后就不能与之前的数据共享幂等性。 个人认为在消息者端实现幂等性，可以最大程度避免重复消费。 事务 事务Producer可以保证跨会话和跨分区的幂等性。和幂等性设置一样enable.idempotence = true，设置Producer端transcational.id,业务代码中也要手动开启和提交事务。在Consumer端设置isolation.level = read_committed。一般是在Kafka Streams 流处理中使用，保证精确的一次语义。平时一般不会使用因为性能不太高。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"Kafka消息无丢失/重复消费配置","slug":"Kafka消息无丢失配置","date":"2019-04-06T01:00:00.000Z","updated":"2019-08-06T19:40:45.506Z","comments":true,"path":"2019/04/06/Kafka消息无丢失配置/","link":"","permalink":"https://jiarus.github.io/2019/04/06/Kafka消息无丢失配置/","excerpt":"","text":"消息丢失/重复消费的场景： 提交消息失败 使用producer.send(msg)提交消息。因为没有回调结果，这时可能消息broker因为网络波动并没有收到，此时消息就丢失了。所以建议使用有回调函数的producer.send(msg,callback)。 Broker端丢失消息 消费者丢失/重复消费消息 offset超前，超过了HighWater（真实已消费的位置）。再次消费会从offset位置开始，中间的消息就丢失了。相反offset落后与HW就导致重复消费。 自动提交offset。可能你使用了多线程处理消息并且是自动提交。如果某个线程处理失败，并且没有显示地通知那么自动提交后就会丢失消息。 最佳实践： 1～3 producer端参数 4～8 broker端参数 1.不使用producer.send(msg)提交消息，一定使用带有回调函数方式提交。 2.使用acks = all,意味着在ISR中的所有的副本broker都接收消息才认为提交成功。 3.设置producer端的retries值&gt;0，即设置重试次数。 4.设置unclean.leader.election.enable=false禁止落后太多的副本选举成为leader，unclean leader指的是落后于最新消息的节点，如果它被选作leader那就肯定会丢失数据。有利有弊，这个参数发挥作用的情况只有当ISR(副本集合)中没有副本，才会执行unclean选举，如果不选举那么就会导致整个broker挂掉，失去高可用性。但一般还是要禁止掉，同时增加多个replicas(副本)个数来保证ISR中有正常的副本。 5.设置replication.factor=3副本个数，可以冗余消息到其他broker上。 6.设置min.insync.replicas&gt;1,设置消息要写入多少个副本才算成功。生产环境最好要&gt;1。 7.确保replication.factor &gt; min.insync.replicas 即副本个数大于需成功写入的个数。生产环境设置replication.factor = min.insync.replicas + 1这样可以保证有一个副本挂掉的情况下仍然可以提交数据。 8.设置enable.auto.commit = false 即设置手动提交offset方式，对单consumer多线程的情况很关键。 在提交消息时，也建议同时使用异步提交+同步提交的策略。保证性能和安全性！ 123456789101112131415 try &#123; while(true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(1)); process(records); // 处理消息 commitAysnc(); // 使用异步提交规避阻塞 &#125;&#125; catch(Exception e) &#123; handle(e); // 处理异常&#125; finally &#123; try &#123; consumer.commitSync(); // 最后一次提交使用同步阻塞式提交 &#125; finally &#123; consumer.close(); &#125;&#125;","categories":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"Kafka分区机制原理","slug":"Kafka分区最佳实践","date":"2019-03-02T15:57:00.000Z","updated":"2019-08-11T02:38:54.482Z","comments":true,"path":"2019/03/02/Kafka分区最佳实践/","link":"","permalink":"https://jiarus.github.io/2019/03/02/Kafka分区最佳实践/","excerpt":"","text":"kafka消息组织结构：主题-分区-消息一、为什么分区？ 提供负载均衡、动态伸缩的能力 支持局部消息顺序消费 二、分区策略 默认分区策略 轮询（Round-robin）是javaProducerAPI默认策略 随机（Randomness） 按消息键保存。自定义每条消息的消息键，消息键代表业务类型，使相同类型的业务放到同一个partition。因为一个分区只能针对同一个消费者，那么该消费者的消息就是有序的。 12345678 #按key类型划分 List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); return Math.abs(key.hashCode()) % partitions.size(); #按地区分区,同理可以按Ip地域划分partition。 List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); return partitions.stream().filter(p -&gt; isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get(); 自定义分区策略 需要实现org.apache.kafka.clients.producer.Partitioner接口，设置并设置partitioner.class值为该实现类全路径类名","categories":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"重写equals和hashCode引发的思考","slug":"关于重写equals","date":"2019-01-28T04:47:00.000Z","updated":"2019-08-10T05:56:23.726Z","comments":true,"path":"2019/01/28/关于重写equals/","link":"","permalink":"https://jiarus.github.io/2019/01/28/关于重写equals/","excerpt":"","text":"关于重写equals方法 最近小伙伴问了我个问题：在Set里存放对象，如果有两个对象属性相同，那么怎么能保证只存在一个对象？HashSet是基于HashMap实现的，所以要看看HashMap的源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //判断该hashCode的key是否存在 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果该hashCode存在，调用equals比较key的值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 可以看到，HashMap是根据hashCode来确定在Node数组中的位置，那么要使HashSet能够对对象去重，就首先需要重写对象的Hash方法，使相同值的对象的HashCode相等，其次还需要重写equals方法，因为即使hashCode相同还是会存放到Map中，这种情况属于Hash冲突，会使用链表存放该对象。 HashCode和equals的关系？ 以下是我的理解 equals比较变量或者对象是否“相同”，这个相同是偏向于业务上的相同，和人理解的“相同”是一个概念。计算机判断的“相同”是hashCode是一致，但如果一个对象,那么计算机并不知道怎么判断他们是否相同(总不能比较对象字节流吧。。)，那么就只能使用Object的hashCode方法判断值是否一致。如果没重写就会导致计算机认为是不同的对象。所以就有了重写equals()一定要重写hashcode()的说法了。但注意hashCode相同equals不一定要相同，因为这个是由于hash算法的优劣决定的。 1234567//下面是Object的生成hashCode方法，根据内存地址生成/** This is stated explicitly here because it is important for implementations to understand that equals() and hashCode() must absolutely, positively work properly -- i.e., two Address objects representing the same address are both equal (via equals()) and have the same hash code. */ public int hashCode();","categories":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/tags/java/"}],"author":"fever"},{"title":"@Transcational注解原理","slug":"Transcational注解原理","date":"2019-01-18T11:20:00.000Z","updated":"2019-08-10T06:59:43.130Z","comments":true,"path":"2019/01/18/Transcational注解原理/","link":"","permalink":"https://jiarus.github.io/2019/01/18/Transcational注解原理/","excerpt":"","text":"@Transcational注解原理 运行配置@Transactional注解的测试类的时候，具体会发生如下步骤 事务开始时，通过AOP机制，生成一个代理connection对象，并将其放入DataSource实例的某个与DataSourceTransactionManager相关的某处容器中。在接下来的整个事务中，客户代码都应该使用该connection连接数据库，执行所有数据库命令(不使用该connection连接数据库执行的数据库命令，在本事务回滚的时候得不到回滚) 事务结束时，回滚在第1步骤中得到的代理connection对象上执行的数据库命令，然后关闭该代理connection对象 根据上面所述，我们所使用的客户代码应该具有如下能力： 每次执行数据库命令的时候如果在事务的上下文环境中，那么不直接创建新的connection对象，而是尝试从DataSource实例的某个与DataSourceTransactionManager相关的某处容器中获取connection对象；在非事务的上下文环境中，直接创建新的connection对象 每次执行完数据库命令的时候如果在事务的上下文环境中，那么不直接关闭connection对象，因为在整个事务中都需要使用该connection对象，而只是释放本次数据库命令对该connection对象的持有；在非事务的上下文环境中，直接关闭该connection对象 @Transactional 通常在哪里使用？ Spring团队的建议是你在具体的类（或类的方法）上使用 @Transactional 注解，而不要使用在类所要实现的任何接口上。你当然可以在接口上使用 @Transactional 注解，但是这将只能当你设置了基于接口的代理时它才生效。因为注解是 不能继承 的，这就意味着如果你正在使用基于类的代理时，那么事务的设置将不能被基于类的代理所识别，而且对象也将不会被事务代理所包装（将被确认为严重的）。因 此，请接受Spring团队的建议并且在具体的类上使用 @Transactional 注解。 可以在类或者方法上使用，不推荐在接口上使用。","categories":[{"name":"spring","slug":"spring","permalink":"https://jiarus.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://jiarus.github.io/tags/spring/"}],"author":"fever"},{"title":"Redis Debug 命令","slug":"Redis-Debug-命令","date":"2019-01-10T06:45:00.000Z","updated":"2019-08-11T13:08:02.055Z","comments":true,"path":"2019/01/10/Redis-Debug-命令/","link":"","permalink":"https://jiarus.github.io/2019/01/10/Redis-Debug-命令/","excerpt":"","text":"查看某一个Key的Debug信息，使用debug object [key] 查看redis内存使用情况使用info memeroy used_memory_human: 已使用内存（格式化可读性的） used_memory_peak_human: 使用最高峰值 total_system_memory_human: 总内存 used_memory_rss_huamn: Redis进程占用内存 evicted_keys: 因为内存满而被驱逐/回收的key","categories":[],"tags":[],"author":"fever"},{"title":"Mysql事务隔离级别","slug":"Mysql事务隔离级别","date":"2018-11-02T11:30:00.000Z","updated":"2019-08-02T12:18:40.545Z","comments":true,"path":"2018/11/02/Mysql事务隔离级别/","link":"","permalink":"https://jiarus.github.io/2018/11/02/Mysql事务隔离级别/","excerpt":"","text":"Mysql事务隔离级别假如你从Mysql设计师的角度看，在事务处理中可能会出现下面几种问题：1.更新丢失 A、B线程并发修改同一条数据，A修改成功并提交，B回滚了修改，这种情况那么也会把A提交的结果也回滚了。A更新的内容就丢失了。（目前的关系型数据库都不会出现这种问题了） 2.脏读 A、B线程并发修改同一条数据，A修改后未提交，B此时读会就会读到A提交前的数据，A也可能会回滚导致B结果错误。 3.不可重复读 A、B、C线程并发修改同一条数据，AB修改后提交，C可能分别读到三个不同的结果。 4.幻读 A、B线程并发修改同一条数据，A添加或者删除并提交了数据，B在A提交前和提交后读到了不同的行数的数据。 正因为如此，设计师在数据上设计了4中事务隔离级别,数据库默认隔离级别是可重复度,可重复读使用了MVCC实现。而幻读可以使用间隙锁解决。 脏读 不可重复读 幻读 读未提交(read-uncommitted) YES YES YES 读已提交(read-commmitted) NO YES YES 可重复读(repeatable) NO NO YES 可串行化(serializable) NO NO NO","categories":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/tags/mysql/"}],"author":"fever"},{"title":"Kafka参数配置(broker、topic、jvm)","slug":"Kafka参数配置-broker、topic、jvm","date":"2018-09-25T05:58:00.000Z","updated":"2019-08-04T19:59:37.771Z","comments":true,"path":"2018/09/25/Kafka参数配置-broker、topic、jvm/","link":"","permalink":"https://jiarus.github.io/2018/09/25/Kafka参数配置-broker、topic、jvm/","excerpt":"","text":"broker中有大概200多个参数，挑选几个重要的记录下来： 1234567log.dirs = /data/kafka1,/data/kafka2,/data/kafka3 #建议每个目录都挂在不同的硬盘上，提高读写性能，也能够支持故障转移zookeeper.connect = zk1:2181,zk2:2181,zk3:2181/kafka1 #最后添加斜杠组名，使zk支持管理多个kafka集群。listeners:#PLAINTEXT 表示明文传输、SSL表示使用SSL或TLS加密传输advertised.listenners:#表明该broker对外发布#都使用主机名，不建议使用IP，因为Broker源代码中使用的是主机名，IP可能导致不能访问。 Topic 12345auto.create.topics.enable：false #是否允许自动创建Topic,建议设置为false，不是设置有可能代码中写错了名称就自动创建错误的Topic。unclean.leader.election.enable:false #是否允许Unclean Leader选举,不允许数据不完整的副本竞选Leaderauto.leader.rebalance.enable：false #是定期选举Leader，没有性能收益。retention.ms:规定Topic中消息存放的时间，覆盖Broker中的留存时间，默认7天retention.bytes:规定Topic中消息的大小，覆盖Broker中的存放数据大小，默认-1 数据存放 123log.retention.&#123;hour|minutes|ms)#设置Broker数据留存的时间，一般设置hourlog.retention.byte#设置Broker存放数据的大小，-1不限制message.max.bytes#设置允许单条message的大小，默认1M，建议改成5M JVM 1234567堆大小默认1G，建议手动修改为6G,因为KakfaBroker与客户端交互会在堆上创建大量的bytebufferKAFKA_HEAP_OPTS：指定堆大小垃圾回收器，Java8设置为G1回收。KAFKA_JVM_PERFORMANCE_OPTS：指定垃圾回收器$&gt; export KAFKA_HEAP_OPTS=--Xms6g --Xmx6g$&gt; export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true$&gt; bin/kafka-server-start.sh config/server.properties 操作系统 文件描述符限制 1ulimit -n 1000000 文件系统类型 这里的文件系统类型指的是如ext3,ext4或XFS日志型文件系统，生产环境最好使用XFS类型。 Swappiness 按照尽量少使用交换区的原则设置 vm.swappiness=1 提交时间 kafka向页缓存（Page cache）写入数据即视为写成功来，并不是等落盘才算成功。页缓存会定时刷到文件中，间隔默认5秒可以适当增加这个间隔。kafka软件层做了多副本冗余，不用太担心宕机丢失数据。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"Kafka线上部署方案","slug":"Kafka线上部署方案","date":"2018-09-20T04:33:00.000Z","updated":"2019-08-06T18:54:51.137Z","comments":true,"path":"2018/09/20/Kafka线上部署方案/","link":"","permalink":"https://jiarus.github.io/2018/09/20/Kafka线上部署方案/","excerpt":"","text":"1.操作系统选择 Linux/Windows/MacOS 三种操作系统kafka最好的搭档还是Linux，原因有几个方面 a.I/O模型 kafka客户端的I/O使用Java的Selector实现，Selector在Linux上的实现机制是epoll，在Windows上是select。性能上不如Linux。b.数据传输效率 Linux支持零拷贝（ZeroCopy）,而Windows直到Java 8的60更新版本 才支持这个功能。 零拷贝:把内核空间地址和用户空间的虚拟地址映射到同一个物理地址，这样DMA就可以填充对内核和用户空间进程同时可见的缓冲区了。省去了内核与用户空间的往来拷贝，java也利用操作系统的此特性来提升性能。 c.社区的支持度 Kafka官方基本上不修复Windows存在的Bug2.磁盘选择 kafka基本上是顺序读写，机械硬盘就可以支持，不需要固态硬盘3.磁盘容量 4.集群数量","categories":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://jiarus.github.io/tags/kafka/"}],"author":"fever"},{"title":"JVM内存模型和垃圾回收策略","slug":"JVM基础知识","date":"2018-09-16T19:50:00.000Z","updated":"2019-08-09T21:58:43.570Z","comments":true,"path":"2018/09/17/JVM基础知识/","link":"","permalink":"https://jiarus.github.io/2018/09/17/JVM基础知识/","excerpt":"","text":"内存模型结构： JVM内存模型主要有图中的5块组成：方法区、堆（这两块是线程共享的）、栈、本地方法栈、程序计数器（这三块是线程私有的） 方法区：是线程共享的区域，用于存放类的模版信息、常量、静态变量、字段、方法等。具体实现：jdk1.7和之前都是永久代（Perm区），当永久代满了就会触发FullGC,影响整个系统。JDK1.8时取掉了永久代，改为了元空间(Metaspace)，元空间数据不再存放在JVM中而是存放到本地内存中。GC效率提高了。空间不足抛出OutOfMemoryError。 堆：也是线程共享的区域，存放java类的实例。具体可分为新生代和老年代，新生代又包括Eden区、survivor0、survivor1区，是GC主要管理的区域。空间不足抛出OutOfMemoryError。 为什么有两个survivor区？ 个人认为是复制算法决定的，需要一个空的内存区来存放存活的对象，这样才能清理数据碎片。 栈：是线程私有区域。包括了局部变量区、操作数栈、动态链接、方法出口信息。栈中可以包含多个栈帧，即一个线程内的调用多个方法开辟多个栈空间。空间不足抛出StackOverflowError。 本地方法栈：存放本地的C或C++方法。比如public static native void sleep(long millis) throws InterruptedException;这就是调用C编写的方法。 程序计数器：因为CPU运行时会切换，需要记录每个线程执行的指令地址。 垃圾回收策略：* 垃圾对象判定标准： 引用计数法：记录对象被引用的次数，引用一次加一，引用失效就减一，引用计数为0即为失效对象。目前新版本JDK不再使用，因为无法判断循环引用的对象是否过期。 根搜索法：构建一个GCRoots为根的树，从根开始搜索，不在树中的即为失效对象。 垃圾回收算法： 标记清除。对活跃对象标注，清除没有标注的对象。清除后空间内对象不再连续，会造成数据内存碎片的问题。 复制。对活跃对象标注，复制到新的空间，再清空老空间的数据。虽然解决了数据碎片，但是清理时会消耗内存空间。 标记整理。是对标记清除的优化，标记清除完数据，在对空间内的存活对象整理向前移动，使之连续。解决内存碎片的问题。 分代回收。将JVM堆分为新生代和老年代，不同的代使用不同的回收算法。新生代因为失效的比较多，所以使用复制算法。老年代因为存活率比较高，而且占用空间大也没有额外的空间分配，所以采用标记清除和标记整理算法。 JVM垃圾回收器具体实现：在生产环境一般在不同的代使用不同的垃圾回收器，图上显示不同回收器针对的内存代 并行垃圾回收器（Parallel Garbage Collector） 年轻代垃圾回收器。使用复制算法。 并发标记扫描垃圾回收器（CMS Garbage Collector） 老年代收集器。使用标记清除算法，GC可以和用户线程并发执行，牺牲了部分性能，减少停顿时间。缺点：GC时加重CPU压力影响性能，造成严重的内存碎片化。 G1垃圾回收器（Garbage First Collector） 使用标记整理+复制。JDK7正式发布，在JDK9中成为默认的垃圾回收器。手动设置：-XX:+UseG1GC。最主要就会有碎片整理的功能。其他的有时间深入研究下2333… FullGC FullGC发生的条件： 年老代（Tenured）被写满； 持久代（Perm）被写满； System.gc()被显示调用； 上一次GC之后Heap的各域分配策略动态变化；","categories":[{"name":"JVM","slug":"JVM","permalink":"https://jiarus.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://jiarus.github.io/tags/JVM/"}],"author":"fever"},{"title":"Java类生命周期","slug":"Java类生命周期及底层调用","date":"2018-09-16T19:49:00.000Z","updated":"2019-08-09T21:24:11.616Z","comments":true,"path":"2018/09/17/Java类生命周期及底层调用/","link":"","permalink":"https://jiarus.github.io/2018/09/17/Java类生命周期及底层调用/","excerpt":"","text":"加载 先说下类加载器（ClassLoader）:java类经过编译生产.class文件。JVM通过类加载器读取类的二进制流，然后在做其他操作。类加载器主要有引导类加载器、扩展类加载器、应用程序类加载器。每次加载一个用户的类都是先判断是否加载，过然后逐层请求父类加载器去获取，如果父类获取不到该类，则交给子类去加载。这种模式称为双亲委派模型。好处：1.避免重复加载。2.保证系统安全性，防止被核心库被替换。 使用类加载器来加载。将类信息存放到元空间，然后在堆中实例化Class对象，最为元空间类的入口。 连接 验证 验证类是否符合java规范。 准备 为变量赋默认值。只有final+static修饰的变量才会赋显示的初始化值。其他变量都只会赋默认值。1234//这里只有num3会赋值1，其他都是0private static int num = 1;private static int num2;private static final int num3 = 1; 解析 将常量池中的符号引用替换为直接引用的过程。 初始化 只有主动使用类才会执行初始化。1234567891011//初始化顺序父类--静态变量父类--静态初始化块子类--静态变量子类--静态初始化块父类--变量父类--初始化块父类--构造器子类--变量子类--初始化块子类--构造器 使用 卸载","categories":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/tags/java/"}],"author":"fever"},{"title":"Redis缓存失效引发的问题","slug":"Redis缓存失效的问题","date":"2018-09-14T13:34:00.000Z","updated":"2019-08-04T14:12:01.821Z","comments":true,"path":"2018/09/14/Redis缓存失效的问题/","link":"","permalink":"https://jiarus.github.io/2018/09/14/Redis缓存失效的问题/","excerpt":"","text":"项目中使用redis作为缓存服务时，当redis中存放的key过期，或者不存在缓存时候可以会引发各种问题： 1.缓存穿透 特指故意构建redis中不存在的key，使请求直接落到数据库层。 解决办法： 1.使用redis构建布隆过滤器，提前将存在的key放入，每次现在过滤器中查找是否存在key，不存在就视为非法的key,不继续进行查找。 2.缓存null值，数据库查询结果为空也缓存在redis，但过期时间要设置短一点。 2.缓存击穿 指redis中某个key过期导致请求落到数据库层，就像在缓存上打了个洞。 解决办法： 1.使用互斥锁。如果redis是单机可以使用synchronized或者lock实现，集群可以使用分布式锁setnx。只允许一个线程去执行查询数据库的操作然后缓存结果，其他线程等待并重试。(我觉得这种方式比使用队列要好) 2.提前预判过期。在value中设置过期时间，比真实的ttl提前一点，这样每次使用redis查询完，判断value中的过期时间是否过期。提前增加ttl的时间。 3.最暴力也是简单的设置key永不过期。 3.缓存雪崩 指redis中部分key集中过期，使大量请求落到数据库层，对数据库造成极大压力，可能压垮数据库。 解决办法： 1.key过期时间生成使用随机因子，不使用相同的过期时间，防止集体过期。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://jiarus.github.io/tags/redis/"}],"author":"fever"},{"title":"Mysql存储引擎区别","slug":"Mysql引擎","date":"2018-08-29T16:24:00.000Z","updated":"2019-07-31T16:54:03.327Z","comments":true,"path":"2018/08/30/Mysql引擎/","link":"","permalink":"https://jiarus.github.io/2018/08/30/Mysql引擎/","excerpt":"","text":"Mysql存储引擎类型及特点： MyiSam 5.6以前默认的存储引擎，非事务，支持Btree、空间、全文索引。 叶子节点指向的是物理存储位置，不需要进行二次查找，所以查询速度快。 使用表锁🔒。 （挖坑，以后写一篇关于索引的） CSV 生成CSV格式文件存储，非事务，不支持索引 Archive 只支持查询和新增，不能修改数据，非事务，只能在自增列建立索引。 用于归档数据/日志 Memory 存放在内存中，速度快，重启丢失，支持Btree,Hash索引 InnoDB 支持ACID,5.6之后的默认存储引擎。使用行级锁🔒。支持自适应Btree，和Hash索引 NDB 集群数据库使用的引擎，支持事务，不常用","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/tags/mysql/"}],"author":"fever"},{"title":"InnoDB事务和MVCC实现","slug":"InnoDB事务实现","date":"2018-08-25T11:54:00.000Z","updated":"2019-07-31T17:33:17.939Z","comments":true,"path":"2018/08/25/InnoDB事务实现/","link":"","permalink":"https://jiarus.github.io/2018/08/25/InnoDB事务实现/","excerpt":"","text":"事务（ACID）定义： Atomicity 一组sql要么全部都执行成功要么全部失败。比如A、B两人转账，扣款和增加同时成功或者失败。 Consistency 写入的结果和预设结果相同。比如转账前和转账后AB两人总额是不变的。 Isolation 事务对其他事务是不可见的，而且互不影响。 Durability 一旦提交，就算宕机也不会出现数据丢失的情况。 事务实现： 依靠undo_log和redo_log实现，如果发生失败就回滚到undo_log,成功就提交redo_log。undo_log实现了事务的原子性，redo_log实现了事务的一致性和持久性。而事务的隔离性依靠加锁来实现。（挖坑，再写一篇事务隔离级别） undo_log记录的是事务执行前的数据 redo_log记录的是事务执行后的数据 数据库锁包括共享锁（S）和排他锁(X) 举个栗子：A:1000元 B:1000元，两人转账时底层是如何操作的 1. undo记录A=1000元,set A=500元(在内存中进行),redo记录A=500元 2. undo记录B=1000元,set B=1500元(在内存中进行),redo记录B=1500元 3. 成功：刷新redo日志到数据库中，失败刷新undo日志到数据库 4. commit事务 多版本并发控制(MVCC)因为数据中存在共享锁和排他锁，而二者是互斥的关系。在查询时会获得共享锁，修改时会获取排他锁，这按理来说会阻塞。但Mysql做了一个处理使得二者可以同时进行。 当进行修改时，生成了undo和redo日志，而并发读读的就是这个undo日志，所以读到的是修改前的数据。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/tags/mysql/"}],"author":"fever"},{"title":"Mysql在线修改表结构","slug":"Mysql在线修改表结构","date":"2018-08-20T10:43:00.000Z","updated":"2019-07-31T16:53:18.484Z","comments":true,"path":"2018/08/20/Mysql在线修改表结构/","link":"","permalink":"https://jiarus.github.io/2018/08/20/Mysql在线修改表结构/","excerpt":"","text":"如果在线修改InnoDB的表结构 在线修改指的是在数据库运行过程中，修改InnoDB表的结构，这样可能会造成锁表的情况发生，就会影响数据的查询插入。所以在生产环境要谨慎修改表结构。 什么情况不支持？1.加全文、空间索引 2.删除主键 3.增加自增列 4.修改列类型 5.修改表字符集 等… 如何安全地在线修改表结构？使用PerconaTookit工具包中的pt-online-schema-change命令 实际修改过程： 建立修改后的结构的表 在原表上创建三个trigger（insert、update、delete） 分批将原表数据copy到新表中 重命名新表并删除旧表","categories":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/tags/mysql/"}],"author":"fever"},{"title":"Mysql索引数据结构","slug":"Mysql索引数据结构","date":"2018-08-18T13:09:00.000Z","updated":"2019-08-01T14:49:33.779Z","comments":true,"path":"2018/08/18/Mysql索引数据结构/","link":"","permalink":"https://jiarus.github.io/2018/08/18/Mysql索引数据结构/","excerpt":"","text":"索引定义： 排好序的数据结构，用来查询时更快找到结果 索引类型1.B+Tree 默认使用B+Tree，对于频繁访问的数据会在建立自适应hash索引，即在B树索引基础上建立hash索引，可以显著提高查找效率，对于客户端是透明的，不可控制的，隐式的 2.Hash 索引的数据结构：尽管有多种数据结构可以选择，但是各有弊端 1.二叉树 不能够做到树节点平衡，如果是自增索引插入，那么就完全退化成了链表结构，时间复杂度在[O(logN),O(N)]之间 2.红黑树 不能控制树的高度，不支持大量数据存储，如果有100W数据那么树的高度= log2100w（树的高度等于查询时的IO次数），时间复杂度为O(logN)。 3.Hash表 范围查询只能按顺序查，范围查找O(N),精确查找O(1)。 4.B-Tree(读作B树) 多路平衡查找树，索引值存放在非叶子节点或叶子节点，所有的索引值只会在树中出现一次。范围查询时候，如果数据在不同的叶子节点，那么就需要查多次。 5.B+Tree 基于B-Tree改进的，叶子节点存放了所有索引值，相邻叶子有双向指针，非叶子节点冗余了部分索引值。 为什么不建议Mysql单表数据超过2000W？因为Mysql索引使用B+Tree,而树的每个节点官方默认为16k，每个节点有个度（Degree）的最大数量，这个最大数量指的是这个节点可以存放的索引的个数，一个索引假设8byte+6byte(指针)=14bye，那么只能存放1170个索引。一般索引层数在1～4之间，如果是三层结构，可存放的索引数量= 1170x1170x16(叶子节点每条数据1k)~= 2000w。如果数据过过会导致树裂变为更多层增加IO次数。 为什么InnoDB要建立整型自增主键？ 如果我们定义了主键(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引、如果没有显式定义主键，则InnoDB会选择第一个不包含有NULL值的唯一索引作为主键索引、如果也没有这样的唯一索引，则InnoDB会选择内置6字节长的ROWID作为隐含的聚集索引(ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的) 如果我们使用了随机的UUID来作为主键，因为是无序的，如果某个节点达到来16k,这样事实上在聚簇数据上会容易产生B+Tree的分裂、也容易浪费系统性能。 联合索引底层存储结构？联合索引和单列索引相似，但是节点里的索引是（a,b,c）的组合，匹配时从左匹配，只要最左的索引使用了，就能使用联合索引，(a,c)这样也能使用但是因为范围大所以没有(a,b)速度快。 为什么Myisam比InnoDB速度快？","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/tags/mysql/"}],"author":"fever"},{"title":"Redis内存淘汰机制","slug":"Redis内存淘汰机制","date":"2018-08-05T06:37:00.000Z","updated":"2019-08-10T06:44:52.412Z","comments":true,"path":"2018/08/05/Redis内存淘汰机制/","link":"","permalink":"https://jiarus.github.io/2018/08/05/Redis内存淘汰机制/","excerpt":"","text":"Redis 内存淘汰机制 (6种) 首先，需要设置最大内存限制 maxmemory 100mb maxmemory设置为0表示不限制Redis内存使用 通常来讲实际内存达到最大内存的3/4时就要考虑加大内存或者拆分数据了 选择策略 maxmemory-policy volatile-lru 解释： noeviction:不淘汰，如果内存已满，添加数据是报错。 allkeys-lru:在所有键中，选取最近最少使用的数据抛弃。 volatile-lru:默认策略，在设置了过期时间的所有键中，选取最近最少使用的数据抛弃。 allkeys-random: 在所有键中，随机抛弃。 volatile-random: 在设置了过期时间的所有键，随机抛弃。 volatile-ttl:在设置了过期时间的所有键，抛弃存活时间最短的数据。","categories":[{"name":"redis","slug":"redis","permalink":"https://jiarus.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://jiarus.github.io/tags/redis/"}],"author":"fever"},{"title":"Java动态代理实现原理","slug":"java动态代理","date":"2018-07-30T15:46:00.000Z","updated":"2019-07-30T17:45:37.834Z","comments":true,"path":"2018/07/30/java动态代理/","link":"","permalink":"https://jiarus.github.io/2018/07/30/java动态代理/","excerpt":"","text":"Java动态代理（dynamic proxy）实现原理应用场景： spring切片、权限、日志、事务管理、Mapper接口等 当你在使用这些spring集成好的功能是否也会疑惑，为什么加个注解，或者按规则创建好文件就可以实现各种功能了呢？ 一、动态代理定义顾名思义，根据源文件方法，动态生成代理类来帮助处理业务。那什么是代理类呢？为什么要生成代理类呢？为什么要动态生成呢？此中缘由请听我慢慢道来。 1.比如说你想去买火车票，但火车站很远而且人也多不方便，就可以到就近的火车票代售点购买车票，代售点就是这里所说的代理。 代理模式就是在访问类目标方法上加了一层业务处理，这层会做一些增强业务，但最终还是会调用目标方法。对外开放的是代理类，而不是原有的类。2.在使用Spring框架的AOP特性时，使用的就是使用动态代理。之所以是动态，是在代码运行阶段在内存里生成代理类，而不是生成文件。否则一个项目里那么多的要代理的类，都生成代理类文件会导致工程庞大冗杂。 二、动态代理的实现动态代理两种实现方式： 1.JDK动态代理实现 特点 1) 使用反射机制 2) 实现InvocationHandler，在invoke方法添加业务逻辑，使用Proxy.newProxyInstance产生代理对象 3) 代理类和实现类实现同一个接口，代理类有一个实现类的引用，客户调用代理类的方法时，代理类就调用实现类的方法 123456789ElectricCar car = new ElectricCar(); ClassLoader classLoader = car.getClass().getClassLoader(); Class&lt;?&gt;[] interfaces = car.getClass().getInterfaces(); InvocationHandlerImpl invocationHandler = new InvocationHandlerImpl(car); Object proxy = Proxy.newProxyInstance(classLoader, interfaces, invocationHandler); Vehicle vehicle = (Vehicle) proxy; vehicle.drive(); Battery battery = (Battery) proxy; battery.charge(); 2.CGlib动态代理实现 特点 1) 使用ASM字节码处理框架 ASM 是一个 Java 字节码操控框架。它能够以二进制形式修改已有类或者动态生成类。ASM 可以直接产生二进制 class 文件，也可以在类被加载入 Java 虚拟机之前动态改变类行为。ASM 从类文件中读入信息后，能够改变类行为，分析类信息，甚至能够根据用户要求生成新类 2) 实现 MethodInterceptor方法代理接口，创建代理类 3) 通过继承原有方法并重写，代理类是原有类的子类，因此不能代理final修饰的方法 12345678public static void main(String[] args) &#123; ProxyDriver proxyDriver = new ProxyDriver(); Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(Verhicle.class); enhancer.setCallback(proxyDriver); Verhicle proxy = (Verhicle) enhancer.create(); proxy.drive(); &#125; 三、动态代理的选择 在 Spring Boot 2.0 中，Spring Boot现在默认使用CGLIB动态代理(基于类的动态代理), 包括AOP。 如果需要基于接口的动态代理(JDK基于接口的动态代理) , 需要设置spring.aop.proxy-target-class属性为false。","categories":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://jiarus.github.io/tags/java/"}],"author":"fever"},{"title":"redis六种基本数据结构","slug":"Redis总结","date":"2018-07-30T04:19:00.000Z","updated":"2019-08-03T14:55:52.549Z","comments":true,"path":"2018/07/30/Redis总结/","link":"","permalink":"https://jiarus.github.io/2018/07/30/Redis总结/","excerpt":"","text":"1.sds(simple dynamic string)简单动态字符串 12345struct sdshdr &#123; int len; //buf中已占用空间的长度 int free; //buf中剩余可用空间的长度 char buf[]; //初始化sds分配的数据空间，而且是柔性数组（Flexible array member）&#125; 定义： sds.h定义了字符串的已使用的长度，剩余空间，已经初始化分配的空间大小 sds.c定义了内存申请、回收策略 预分配策略，字符串修改后。如果表头len小于1M，则扩容len相同的空间。free和len大小相同。否则扩容固定大小为1M的空间。 回收策略，惰性空间释放，当要缩短字符串时并不是立即释放全部空间，而是修改头文件len的长度，待之后使用。 优点： 时间复杂度为O(1) 因为表头存放字符串len，可以根据长度直接获取整个字符串 杜绝缓冲区溢出 C语言中使用strcat进行字符串拼接，如果没分配足够的空间就会导致缓冲区溢出。Redis会先判断len长度，自动扩容满足空间需要。 兼容C部分函数 使用字符串/0结尾与C语言相同 二进制安全性 支持存放图片，视频二进制文件。传统字符串当遇到/0就结束了，所以不能存放图片视频二进制文件 2.链表 12345678typedef struct listNode&#123; //前置节点 struct listNode *prev; //后置节点 struct listNode *next; //节点的值 void *value; &#125;listNode 根据listNode定义了一个双端链表 1234567891011121314typedef struct list&#123; //表头节点 listNode *head; //表尾节点 listNode *tail; //链表所包含的节点数量 unsigned long len; //节点值复制函数 void (*free) (void *ptr); //节点值释放函数 void (*free) (void *ptr); //节点值对比函数 int (*match) (void *ptr,void *key);&#125;list; 特点： 双端 无环 len记录了链表长度，获取长度O(1) ListNode中void类型可以存放不同类型的值 3.字典 12345678910111213typedef struct dictEntry&#123; //键 void *key; //值 union&#123; void *val; uint64_tu64; int64_ts64; &#125;v; //指向下一个哈希表节点，形成链表 struct dictEntry *next;&#125;dictEntry 头文件中定义了字典的大小和使用情况 123456789101112typedef struct dictht&#123; //哈希表数组 dictEntry **table; //哈希表大小 unsigned long size; //哈希表大小掩码，用于计算索引值 //总是等于 size-1 unsigned long sizemask; //该哈希表已有节点的数量 unsigned long used; &#125;dictht 特点： 哈希算法 1234#1、使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key);#2、使用哈希表的sizemask属性和第一步得到的哈希值，计算索引值index = hash &amp; dict-&gt;ht[x].sizemask; 使用链表法解决hash冲突 扩容和缩容 扩容会增加一倍空间，缩容会根据已使用空间缩小一倍，两者都会创建新的哈希表。 触发扩容条件 当没有执行BGSAVE或BGREWRITEAOF命令，且负载因子大于等于1 当正在执行BGSAVE或BGREWRITEAOF命令，且负载因子大于等于5 负载因子 = 哈希表已使用空间/哈希表大小 渐进式rehash 哈希表扩容和缩容并不会一次性将数据导入新的哈希表，而是分多次迁移，保证哈希表的可用性。读取时先查老的哈希表再查新的哈希表，插入一定是插入新的哈希表。 4.跳跃表skiplist 1234567891011121314151617typedef struct zskiplistNode &#123; //层 struct zskiplistLevel&#123; //前进指针 struct zskiplistNode *forward; //跨度 unsigned int span; &#125;level[]; //后退指针 struct zskiplistNode *backward; //分值 double score; //成员对象 robj *obj; &#125; zskiplistNode 多个节点构成跳跃表 123456789typedef struct zskiplist&#123; //表头节点和表尾节点 structz skiplistNode *header, *tail; //表中节点的数量 unsigned long length; //表中层数最大的节点的层数 int level; &#125;zskiplist; 特点： 由多层结构组成 每层都是有序的 上层节点有下层节点的指针，只对应一个，这个是b+Tree有所不同 底层包含所有的数据 5.整数集合 用来保存整数集合，可以存放uint16,uint32,uint64类型,可以保证不会重复 123456789typedef struct intset&#123; //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[]; &#125;intset; 6.压缩列表(ziplist) 特点： 压缩列表并不是对数据做了压缩，而是在像数组一样在连续的内存空间存放数据。 数组必须要每个数据大小都相同,按最大长度的数据作为元素大小。压缩列表可以存放不同大小的数据并且不浪费空间，还有个好处是可以存放不同类型的数据。","categories":[{"name":"redis","slug":"redis","permalink":"https://jiarus.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://jiarus.github.io/tags/redis/"}],"author":"fever"},{"title":"Mysql日志","slug":"Mysql日志","date":"2018-07-21T14:34:00.000Z","updated":"2019-07-31T15:01:59.175Z","comments":true,"path":"2018/07/21/Mysql日志/","link":"","permalink":"https://jiarus.github.io/2018/07/21/Mysql日志/","excerpt":"","text":"Mysql日志简介日志类型： error_log 错误日志，会记录Mysql启动、运行、停止过程中所有的异常信息。 log_error_verbosity = [1,2,3]可以设置错误日志级别 log_error_service是Mysql8.0的新特性，可以设置处理日志的组件，如输出为json格式等 general_log 常规日志，记录运行是所有的SQL操作信息（包含查询），在生产环境上不建议开启 slow_query_log 慢查询日志，根据配置的筛选条件，匹配SQL，一般用来找出执行慢的SQL语句，默认只匹配DML语句，也可以设置DDL语句。 binary_log 二进制日志，记录了处理对表修改成功的如：insert,delete,update的行为，并不会记录select。主要用来主从同步，基于时间点的数据恢复。 开启日志log_bin=/var/lib/mysql/mysql-bin设置binlog开启，并且设置路径设置bin_log格式binlog_format=row二进制日志有三种模式，基于语句(statement)的，基于行的（row），以及结合前两者的混合模式（mixed）,主从同步建议使用row 设置日志刷新时机sync_binlog=1 也可以绑定binlog_do_db/排除binlog-ignore-db开启binlog的库 relay_log 中继日志使用在主从复制的情况，从库会生成relay_log,并且会在同步完成自动删除","categories":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/tags/mysql/"}],"author":"fever"},{"title":"Mysql不同发行版特性及升级步骤","slug":"Mysql不同发行版区别及升级步骤","date":"2018-07-20T09:50:00.000Z","updated":"2019-07-31T11:07:20.159Z","comments":true,"path":"2018/07/20/Mysql不同发行版区别及升级步骤/","link":"","permalink":"https://jiarus.github.io/2018/07/20/Mysql不同发行版区别及升级步骤/","excerpt":"","text":"Mysql发行版 Mysql社区版 Mysql企业版 Percona Mysql MariaDB 基本上就这4种发行版，Mysql社区版和企业版都是官方发行的版本，更新速度比较快，其他两种是基于Mysql官方版的，所以更新会落后与官方发行版。 Mysql社区版我们开发中常用的版本，目前已经更新到8.0了。之前的5.6、5.7版本很多公司都还在使用。5.7之后就直接升级到8.0版本了。 5.6 - &gt; 5.7 1主要提高了主从模式下，多线程复制功能，支持了并发。同时也增加了一些性能指标的监控 5.7 - &gt; 8.0 1234567891011121314151617181.提高了主从模式下，对json字段的复制性能，之前是全量同步，现在支持只同步修改的部分。2.之前创建innodb引擎表时，会创建一个.frm文件(存放创建表的语句)，8.0之后就没有了。不再支持myisam引擎，只支持innodb引擎，数据存放在.idb中使用的独立的表空间。3.支持定义资源组， `create resource group` 用来控制线程的优先级和使用的CPU资源。 -- 以后在写一篇，挖坑4.支持不可见索引（索引不会被优化器处理）和降序索引，支持数据直方图5.支持窗口函数(对于每个组返回多行，而聚合函数对于每个组只返回一行) -- 以后在写一篇，挖坑6.在线修改环境变量，不会导致重置后配置丢失7.安全性上，密码动态加密，密码历史记录，限制使用次数等8.innodb性能大幅度增强 Percona Mysql 12在官方版本的基础上，提高了高负载情况下innodb的性能，也包含了社区版没有，企业版才有的监控、审计日志等的功能。PerconaTookit可以支持在线地修改表结构。 MariaDB 1GTID(主从同步时的事务id)生成规则与其他两兄弟是不一样的，所以在开发中不要混用不同的发行版 升级数据库 1.升级前要了解升级带来的益处，能够解决业务上的什么痛点 2.可能影响哪些业务，带来什么风险 比如升级后jdbc不兼容 SQL mode 变化导致不能正确执行SQL 5.6版本默认打开了PERFORMANCE_SCHEMA(收集数据库服务器性能参数)，这个在5.5是关闭的，会影响一部分性能。 … 3.制定升级方案 升级前备份 关闭老版本MYSQL服务 替换MYSQL二进制文件 执行mysql upgrade命令来检查数据是否兼容 检查业务 …4.制定回滚方案 回滚步骤 检查 …","categories":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://jiarus.github.io/tags/mysql/"}],"author":"fever"},{"title":"redis五大数据类型实现原理","slug":"redis数据结构实现原理","date":"2018-07-03T08:13:00.000Z","updated":"2019-08-03T14:55:18.370Z","comments":true,"path":"2018/07/03/redis数据结构实现原理/","link":"","permalink":"https://jiarus.github.io/2018/07/03/redis数据结构实现原理/","excerpt":"","text":"0.redisObject 12345678910111213typedef struct redisObject&#123; //类型 unsigned type:4; //编码 unsigned encoding:4; //指向底层数据结构的指针 void *ptr; //引用计数 int refcount; //记录最后一次被程序访问的时间 unsigned lru:22; &#125;robj redis的key都是字符串类型，value是redis的包装类型,以下五种类型表现形式都是一个redisObject。 type类型来区分是哪种具体类型。type key encoding用来表明该类型使用哪种编码，而每种类型至少使用了两种编码。OBJECT ENCODING key 1.字符串 a)编码类型 int,embstr,raw三种编码 int保存可用long表示的无符号整数值 embstr表示长度小于44的短字符串 raw表示长度大于44的长字符串 embstr和raw的区别： 1.embstr由于是短字符串，redis做了优化，只分配一次空间redisObject和sds是连续的，而raw是分开的。添加和删除时候embstr都比raw少操作一次空间。 b)编码转换 1.embstr由于没有提供修改方法，需要升级为raw才能修改，修改完即使小于44字节还是会变成raw编码。 2.如果是float类型保存也是按字符串保存的，当需要时候才转换为float。 3.当int编码保存的不再是整数，或超过long的长度就会转换为raw编码。2.列表 存放简单字符串sds的链表结构 a)编码类型 压缩链表（ziplist）和双端链表（linklist） b)编码转换 当满足字符串长度小于64字节并且字符串数量小于512个俩个条件时，使用ziplist,否则使用linklist。3.哈希 值是字符串，value是键值对 a)编码类型 压缩链表（ziplist）和哈希表(hashtable) 哈希表底层使用字典类型结构 b)编码转换 和列表一样，当满足字符串长度小于64字节并且字符串数量小于512个俩个条件时，使用ziplist,否则使用hashtable。4.集合 不重复且无序的列表 a)编码类型 intset和哈希表（hashtable） 哈希表存放的只有key，value都指向null.类似与java中的HashSet，使用HashMap实现，value都是null。 b)编码转换 当存放的所有元素都是整数且不超过512个使用intset，否则使用哈希表。5.有序集合 不重复且有序的列表 a)编码类型 压缩链表（ziplist）和 （字典+跳表） 当使用压缩链表实现时，值和分数是相邻存放，前面是值，后面跟的分数，值之间是从小到大有序排列。 当使用字典+跳表实现时，执行精确查找使用字典，范围查找使用跳表，他们二者共享同一份数据。 b)编码转换 当存放的元素长度小于64且个数小于128时使用压缩链表，否则使用字典+跳表实现。","categories":[{"name":"redis","slug":"redis","permalink":"https://jiarus.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://jiarus.github.io/tags/redis/"}],"author":"fever"},{"title":"DSP系统架构","slug":"DSP系统架构","date":"2018-05-09T14:56:00.000Z","updated":"2019-08-10T06:01:48.971Z","comments":true,"path":"2018/05/09/DSP系统架构/","link":"","permalink":"https://jiarus.github.io/2018/05/09/DSP系统架构/","excerpt":"","text":"","categories":[],"tags":[{"name":"广告系统","slug":"广告系统","permalink":"https://jiarus.github.io/tags/广告系统/"}],"author":"fever"},{"title":"Spring事务传播机制和Mysql事务隔离级别关系","slug":"Spring事务传播机制和Mysql事务隔离级别关系","date":"2018-04-25T07:08:00.000Z","updated":"2019-08-10T04:14:38.533Z","comments":true,"path":"2018/04/25/Spring事务传播机制和Mysql事务隔离级别关系/","link":"","permalink":"https://jiarus.github.io/2018/04/25/Spring事务传播机制和Mysql事务隔离级别关系/","excerpt":"","text":"spring 事务类型 1、PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。 2、PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘ 3、PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 4、PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。（失败隔离） 5、PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 6、PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 7、PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 @Transactional 只能被应用到public方法上, 对于其它非public的方法,如果标记了@Transactional也不会报错,但方法没有事务功能 和Mysql事务隔离级别的关系 当Spring开启了事务并设置了传播机制，那么会覆盖Mysql已有的事务隔离级别。如果Mysql不支持该隔离级别，Spring的事务就也不会生效。","categories":[{"name":"spring","slug":"spring","permalink":"https://jiarus.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://jiarus.github.io/tags/spring/"}],"author":"fever"},{"title":"Linux之grep命令","slug":"Linux之grep命令","date":"2018-04-11T11:07:00.000Z","updated":"2019-08-10T07:16:13.688Z","comments":true,"path":"2018/04/11/Linux之grep命令/","link":"","permalink":"https://jiarus.github.io/2018/04/11/Linux之grep命令/","excerpt":"","text":"grep参数 -i 不区分大小写 -E 启用扩展命令grep -E &quot;word1|word2|word3&quot; file.txt -c 统计行数 -P 启用正则表达式 -A 显示匹配后和它后面的n行 -B 显示匹配行和它前面的n行 -C 匹配行和它前后各n行 同时满足多个关键字和满足任意关键字 grep -E “word1|word2|word3” file.txt满足任意条件（word1、word2和word3之一）将匹配。 grep word1 file.txt | grep word2 |grep word3必须同时满足三个条件（word1、word2和word3）才匹配。 12#同时满足两个条件grep -P &apos;\\t724\\t&apos; sspclick.tmp | grep -P &apos;\\t2019051418&apos; |wc -l","categories":[{"name":"linux","slug":"linux","permalink":"https://jiarus.github.io/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://jiarus.github.io/tags/linux/"}],"author":"fever"},{"title":"MyBatis和数据库的交互的两种方式","slug":"MyBatis和数据库的交互的两种方式","date":"2018-01-16T10:15:00.000Z","updated":"2019-08-10T04:17:21.191Z","comments":true,"path":"2018/01/16/MyBatis和数据库的交互的两种方式/","link":"","permalink":"https://jiarus.github.io/2018/01/16/MyBatis和数据库的交互的两种方式/","excerpt":"","text":"MyBatis和数据库的交互有两种方式： 方式一、原始的dao调用mybatis １．需要定义dao接口 ２．需要实现dao接口 ３．在dao接口实现类上注入sqlSessionFactory，创建sqlSessionFactory时需要读取mapper.xml到内存 ４．然后通过创建sqlSession对象来调用mapper中的sql语句（sqlSession.insert(statementId,sql)）,在操作结束后需要手工进行提交，释放资源，返回结果 这种方式存在的问题： （1）dao接口实现类方法中存在大量模板方法，如：通过SqlSessionFactory创建SqlSession，调用SqlSession的数据库操作方法。 (2）调用sqlSession的数据库操作方法需要指定statement的id，这里存在硬编码。 (3）调用sqlsession方法时传入的变量，由于sqlsession方法使用泛型，即使变量类型传入错误，在编译阶段也不报错，不利于程序员开发。 方式二、Mapper动态代理: １．需要定义mapper接口 ２．在定义mapper.xml时，需要遵守几条原则(路径，方法，参数，返回结果都要一致) a.xml中的namespace要和mapper接口的路径相同 b.xml中的statementID要和mapper接口中定义的方法相同 c.xml中的参数类型paramerType要和mapper接口中定义的参数类型相同 d.xml中的返回结果resultType要和mapper接口中定义的返回结果相同 ３．在整合spring时，spring初始化就已经把mapper接口类注入了，在注入过程中实际上使用的是动态代理的方式进行的实例化了。 ４．动态代理：创建动态代理类MapperProxy需要实现InvocationHandler接口，然后把真实被代理类传入。 ５．mybatis代理类的实现过程，会调用动态代理类MapperProxy中invoke方法，接着会调用MapperMethod.execute()，在构造MapperMethod类时，有sqlCommand和MethodSignature两个内部类用来绑定sql和填充参数，然后再调用execute方法最终还是会回到sqlSession上。 这种方式其实是将原始的调用dao的方式优化了，使用动态代理在sqlSession这一步封装了一层，可以不需要传递statementID和参数，而直接通过传递类型来实例化接口。例如：Mapper mapper = getMapper(Mapper.class);","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://jiarus.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://jiarus.github.io/tags/mybatis/"}],"author":"fever"},{"title":"排序算法","slug":"排序算法","date":"2018-01-08T21:28:00.000Z","updated":"2019-08-09T21:48:12.787Z","comments":true,"path":"2018/01/09/排序算法/","link":"","permalink":"https://jiarus.github.io/2018/01/09/排序算法/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * User: fever * Date: 18-1-9 * Time: 下午1:55 */public class MyQuickSort &#123; private int arr[] = &#123;112, 2, 7, 9, 20, 3, 48, 6, 57, 88, 60, 42, 83, 73, 85&#125;; /** * 相当于把比基准值小的放在左边，大的放在右边 * 需要重复多次直到左右两边只有一个数，也就是只有三个数，这样一定是有序的 * * @param arr * @param lo * @param hi * @return */ private int partition(int[] arr, int lo, int hi) &#123; //基准值的索引 int keyIndex = lo; while (lo &lt; hi) &#123; //从右向左 while (arr[keyIndex] &lt; arr[hi] &amp;&amp; lo &lt; hi) &#123; hi--; &#125; // 基准值和hi值互换 // 基准值索引变为hi int tmp = arr[hi]; arr[hi] = arr[keyIndex]; arr[keyIndex] = tmp; keyIndex = hi; //从左向右 while (arr[keyIndex] &gt; arr[lo] &amp;&amp; lo &lt; hi) &#123; lo++; &#125; //基准值和lo值互换 //基准值索引变为lo tmp = arr[lo]; arr[lo] = arr[keyIndex]; arr[keyIndex] = tmp; keyIndex = lo; &#125; return hi; &#125; /** * 继续分区排序 * 左右两部分递归调用分区函数 * * @param arr * @param lo * @param hi */ private void sort(int[] arr, int lo, int hi) &#123; if (lo &gt;= hi) &#123; return; &#125; // 上次分区的位置 int par = partition(arr, lo, hi); // 左边排序 递归。。。直到lo&gt;=hi sort(arr, lo, par - 1); // 右边排序 递归。。。 sort(arr, par + 1, hi); &#125; @org.junit.Test public void test() &#123; sort(arr, 0, arr.length - 1); for (int i = 0; i &lt; arr.length; i++) &#123; System.out.println(arr[i]); &#125; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://jiarus.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://jiarus.github.io/tags/算法/"}],"author":"fever"}]}